{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3564b4cf-772d-4927-898d-11d9329332dc",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14b2607-376a-45c0-965a-656c06dfd941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/Users/leisun/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1437606c721149e498a6878c8c1d1f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"emotion\") # loading emotions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de058fd7-6c19-43e1-ac79-b2936de8de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel # The Auto meant it will automatically select model to use based on model_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ebb5ba-ce7c-4350-a3aa-e78d8b029638",
   "metadata": {},
   "source": [
    "## Tokenising and Embedding\n",
    "\n",
    "This is to use the **distilbert** pre-trained model tokeniser.  Use AutoTokenizer class to automatically determine tokeniser from the name (i.e. model_ckpt). Otherwse can also use the specific tokeniser. See below, the AutoTokenizer returns DistilBertTokenizerFast class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb7f86c-2fb8-4cd4-83c8-88bdf519a66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d6003-b540-4b9f-b540-554fa8d4f2eb",
   "metadata": {},
   "source": [
    "### Padding, Truncation, Tokens & IDs\n",
    "\n",
    "- The device can be set as \"cpu\", \"cuda\" or \"mps\" where mps is for apple silicon\n",
    "- The tokenizer takes a text array and produce a tensor, use \"pt\" for pytorch \"tf\" for tensorflow\n",
    "\n",
    "**To insure texts are at the same size**\n",
    "- Padding meaning text will be padded as 0's, taking the longest text in the array.\n",
    "- Trunction meaning the longer texts will be truncted to context length of the model (max tokens)\n",
    "- [CLS], [SEP] will be introduced at the beginning and end of each sentence\n",
    "\n",
    "#### Example Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e136ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1037, 17162,  3435,  3231,   102,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2023,  2003,  2178,  3231,  1010,  2021,  1037,  2843, 12430,\n",
      "          1998,  2936,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['[CLS]', 'this', 'is', 'another', 'test', ',', 'but', 'a', 'lot', 'slower', 'and', 'longer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\") # utilising apple silicon\n",
    "\n",
    "# this is a sample text\n",
    "text = [\"this is a blazing fast test\", \"this is another test, but a lot slower and longer\"]\n",
    "\n",
    "# this is to apply the tokenizer\n",
    "inputs = tokenizer(text, padding = True, truncation = True, return_tensors = \"pt\") # by default tokeniser returns arrays as tensor (pt is pytorch)\n",
    "\n",
    "print(inputs) # can see the first sentence is padded with zeros at the end. The ids are in the vocab\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][1])) # note that [CLS] [SEP] is added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07b4f0-afad-4e33-8c42-0cc1b0568a12",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "The model is loaded as well, note it is the same model_ckpt. \n",
    "\n",
    "The function will accept input and generate hidden states of the inputs. (hidden states are outputs of recurrent type of networks) \n",
    "\n",
    "Firstly the input (batch) will be converted into a dictionary with value carried to the device for comput. Then it will be used as input to the model. Note by having torch.no_grad(), we save the compute (not needed as we just need to output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4829789b-9045-4cce-990e-349659fe849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "print(tokenizer.model_input_names)\n",
    "\n",
    "inputs_ = {k: v.to(device) for k, v in inputs.items() if k in tokenizer.model_input_names} # carry to device\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_outputs = model(**inputs_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6621fe4b-9f8d-4566-8ead-e95518f1b40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs.last_hidden_state[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14722217-44d7-4173-a90c-b10c1c99b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# this function uses the tokeniser on the batch text (this is the feature of emotion['train']. \n",
    "# this also applies padding\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding = True, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "# this function extract last hidden state from a batch \n",
    "def extract_hidden_states(batch):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() # model only expect input_ids and attention_mask\n",
    "                 if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state # take the last hidden state [CLS]\n",
    "\n",
    "    return {\"hidden_state\": last_hidden_state.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2518cec4-e62f-41af-9d67-fc3e8cde91f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/leisun/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-340b8d5609895ea7.arrow\n",
      "Loading cached processed dataset at /Users/leisun/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-edaf99feeb6ed202.arrow\n",
      "Loading cached processed dataset at /Users/leisun/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-ba243ea584e486d4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(0), 'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched = True, batch_size = None) # do not batch as it will need padding\n",
    "emotions_encoded.set_format('torch', columns = ['input_ids', 'attention_mask', 'label']) # set the ds to use torch\n",
    "                                                \n",
    "print(emotions_encoded['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b357b2-ddab-4408-8be7-aaa3d899656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/leisun/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-5a647c2500552338.arrow\n",
      "Loading cached processed dataset at /Users/leisun/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-1996d999c6672d2b.arrow\n",
      "Loading cached processed dataset at /Users/leisun/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-ac8ec274b73b2b38.arrow\n"
     ]
    }
   ],
   "source": [
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched = True, batch_size = 128) #this needs batch as it will run out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a8f721-470f-4bce-a018-295ab2241a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(emotions_hidden, open(\"./hidden_state.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26092b23-ee23-4c46-9c87-589ade49e3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'input_ids': tensor([  101,  1045,  2064,  2175,  2013,  3110,  2061, 20625,  2000,  2061,\n",
       "          9636, 17772,  2074,  2013,  2108,  2105,  2619,  2040, 14977,  1998,\n",
       "          2003,  8300,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.to(device) for k, v in emotions_encoded['train'][1].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9645d-e021-48a3-85bd-8688526f1c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
