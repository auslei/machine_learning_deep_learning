{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in 2_DNN_TF/C4_NLP_Sentiment_Analysis_RNN.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master f923b5c] initial check in\n",
      " 1 file changed, 249 insertions(+)\n",
      " create mode 100644 2_DNN_TF/C4_NLP_Sentiment_Analysis_RNN.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/auslei/python.git\n",
      "   e38a8c7..f923b5c  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add C4_NLP_Sentiment_Analysis_RNN.ipynb\n",
    "!git commit -m \"initial check in\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"imdb_reviews\", as_supervised = True, with_info = True)\n",
    "train_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['test', 'train', 'unsupervised']),\n",
       " 'Large Movie Review Dataset.\\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       " FeaturesDict({\n",
       "     'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "     'text': Text(shape=(), dtype=tf.string),\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetInfo\n",
    "for t in dataset['test']: print(t);break\n",
    "dataset.keys(), info.description, info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300) # only take the first 300 characters\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \") # remove newlines and brs\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \") # keep only letters\n",
    "    X_batch = tf.strings.split(X_batch) #break into words\n",
    "    return X_batch.to_tensor(default_value = b\"<pad>\"), y_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vocabulary\n",
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in dataset['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3] # top 3 words and their respective count (should really remove stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of words and keep within 10k. this is to construct the word ids\n",
    "vocab_size = 10000\n",
    "trucated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(trucated_vocabulary)\n",
    "word_ids = tf.range(len(trucated_vocabulary), dtype = tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets) #create a look up table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[    9,     7, 10893]], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"this is thisisnotaword\".split()])) #note that the not word is added in the OOV buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = dataset[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 60), dtype=int64, numpy=\n",
      "array([[  22,   11,   28, ...,    0,    0,    0],\n",
      "       [   6,   21,   70, ...,    0,    0,    0],\n",
      "       [4099, 6881,    1, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  22,   12,  118, ...,  331, 1047,    0],\n",
      "       [1757, 4101,  451, ...,    0,    0,    0],\n",
      "       [3365, 4392,    6, ...,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)>)\n"
     ]
    }
   ],
   "source": [
    "for s in train_set: print(s); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 21s 19ms/step - loss: 0.6875 - accuracy: 0.5156\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.4595 - accuracy: 0.7849\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.2773 - accuracy: 0.8910\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.1544 - accuracy: 0.9475\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.1075 - accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape = [None]),\n",
    "    # creates an embedding layer witha embeding size predefined\n",
    "    keras.layers.GRU(128, return_sequences = True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "history = model.fit(train_set, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         1408000   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,606,273\n",
      "Trainable params: 1,606,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "The previous exmaple uses padding which the model will learn to ignore. The is the information we arleady know. To reduce the computational cost, we can use masking to ignore the word/character for all downstream layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "inputs = tf.constant([1,0,0,0])\n",
    "mask = tf.constant([1,1,1,0])\n",
    "\n",
    "K.not_equal(inputs, mask) # returns boolean tensor (T where not equal, False where equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_with_tf",
   "language": "python",
   "name": "ml_with_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
