{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Fashion mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(X_train_full.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation Data Set\n",
    "\n",
    "Also in addition to that do a very basic scaling (ie. dividing data by 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28) (5000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid  = X_train_full[5000:] / 255.0, X_train_full[:5000] / 255.0\n",
    "y_train, y_valid  = y_train_full[5000:], y_train_full[:5000]\n",
    "\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/pop\", \"Trouser\", \"Pullover\", \"Dresss\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same Effect as:\n",
    "model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.Dense(300, activation = \"relu\"),\n",
    "        keras.layers.Dense(100, activation = \"relu\"),\n",
    "        keras.layers.Dense(10, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ausle\\AppData\\Local\\Temp\\ipykernel_7956\\3428846781.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nn.functional.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3023, grad_fn=<NllLossBackward0>)\n",
      "               Layer\tOutput Shape        \t    Kernal Shape    \t#params             \t#(weights + bias)   \trequires_grad\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "           Flatten-1\t[1, 784]            \t                    \t                    \t                    \t          \n",
      "            Linear-2\t[1, 300]            \t     [300, 784]     \t235500              \t(235200 + 300)      \tTrue True \n",
      "            Linear-3\t[1, 100]            \t     [100, 300]     \t30100               \t(30000 + 100)       \tTrue True \n",
      "            Linear-4\t[1, 10]             \t     [10, 100]      \t1010                \t(1000 + 10)         \tTrue True \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Total parameters 266,610\n",
      "Total Non-Trainable parameters 0\n",
      "Total Trainable parameters 266,610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(266610, 266610, 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorchsummary import summary as pt_summary\n",
    "print(torch.__version__)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = nn.functional.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "p_model = Net()\n",
    "optimiser = optim.SGD(p_model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "label = torch.tensor(y_train)\n",
    "input = torch.tensor(X_train).to(torch.float32)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = p_model(input)\n",
    "    loss = criterion(outputs, label)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    print(loss)\n",
    "    \n",
    "pt_summary((1, 28, 28), p_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Model Summary\n",
    "The model summary can be easily viewed by calling **summary()** function. The number of params in Dense Layer is:\n",
    "\n",
    "- Flatten is basically doing np.reshape(-1, 28*28). Changing the dimention of the input observation to 2 dimensions (square to a line)\n",
    "- Dense is a layer of fully connected neurons\n",
    "- The final layer consists of 10 class as output. Placing a softmax at the end. (using argmax to find the predicted class.\n",
    "\n",
    "#Neurons from previous layer times #Neurons of the current layer + #Neurons of the current layer (biases)\n",
    "\n",
    "Additionally each layer can be accessed via **layers** property.\n",
    "\n",
    "Weights can be accessed via **get_weights()** function for each of the layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "\n",
    "x = zip(a,b)\n",
    "for a,b in x:\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 300) (300,)\n"
     ]
    }
   ],
   "source": [
    "weights, bias = model.layers[1].get_weights()\n",
    "print(weights.shape, bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Model\n",
    "\n",
    "Before model can be trained, it needs to be compiled. The key arguments are:\n",
    "\n",
    "- loss functions: for multiclass use sparse_categorical_crossentropy, for binar class, we can use catagorical_crossentroy. (https://keras.io/loss for more details (equivalent to tf.keras.loss.sparse_categorical_crossentropy)\n",
    "- optimizer: for example use SGD, Adam (https://keras.io/optimizers\n",
    "- merics: can specify \"accuracy\" which is equivalent to tf.keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "Also note the dense layers are automatically initialised with random weight (0 bias), there maybe different initialisation should there be problem with converging (due to vanishing gradients) https://keras.io/initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both statements are the same\n",
    "\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=tf.keras.optimizers.SGD(lr = 0.3), \n",
    "#              loss = tf.keras.losses.sparse_categorical_crossentropy, \n",
    "#              metrics = [tf.keras.metrics.sparse_top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "**Validation**\n",
    "Instead passing a validation dataset, we can also pass in a **validation_split** argument. (For example, validation_split = 0.1 tells Keras to use at least 10% of data for validation)\n",
    "\n",
    "**Class Weight**\n",
    "If the classes are skewed, we can use class_weight argument which will give larger weight to underrepresented classes and a lower weight to overrepresented classes. These weigts are used to caculate loss. \n",
    "\n",
    "Can use also sample_weight for per-instance weights. This could be useful if some instances were labeled by *experts* and other classes are labeled via *crowd sourcing* in this case more weight should be in the former. \n",
    "\n",
    "If both weights are specified, they will be mulitiplied.\n",
    "\n",
    "**batch_size**\n",
    "batch_side by default is 32.\n",
    "\n",
    "**evaluate**\n",
    "The model can simply evaluated via a evaluate call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4201 - accuracy: 0.8531 - val_loss: 0.4082 - val_accuracy: 0.8594\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4011 - accuracy: 0.8586 - val_loss: 0.4077 - val_accuracy: 0.8584\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3841 - accuracy: 0.8647 - val_loss: 0.3965 - val_accuracy: 0.8660\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3705 - accuracy: 0.8680 - val_loss: 0.3697 - val_accuracy: 0.8732\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3592 - accuracy: 0.8725 - val_loss: 0.3682 - val_accuracy: 0.8728\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3473 - accuracy: 0.8762 - val_loss: 0.3649 - val_accuracy: 0.8754\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3387 - accuracy: 0.8790 - val_loss: 0.3480 - val_accuracy: 0.8798\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3292 - accuracy: 0.8825 - val_loss: 0.3477 - val_accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3216 - accuracy: 0.8840 - val_loss: 0.3656 - val_accuracy: 0.8686\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3145 - accuracy: 0.8866 - val_loss: 0.3316 - val_accuracy: 0.8806\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "print(X_train.shape)\n",
    "history = model.fit(X_train, y_train, epochs=NUM_EPOCHS, validation_data=(X_valid, y_valid), batch_size = 32)\n",
    "#history = model.fit(X_train_full, y_train_full, epochs=NUM_EPOCHS, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Object\n",
    "\n",
    "Fit function returns a history object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(history\u001b[39m.\u001b[39mhistory)\u001b[39m.\u001b[39mplot(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mgrid(\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 798us/step - loss: 91.0078 - accuracy: 0.7904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[91.00779724121094, 0.7904000282287598]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#model.predict_classes(X_new)\n",
    "y_pred = np.argmax(model.predict(X_test[:3]), axis = -1)\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.725055</td>\n",
       "      <td>0.764273</td>\n",
       "      <td>0.498144</td>\n",
       "      <td>0.8350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.489142</td>\n",
       "      <td>0.830491</td>\n",
       "      <td>0.451548</td>\n",
       "      <td>0.8436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444989</td>\n",
       "      <td>0.844291</td>\n",
       "      <td>0.433190</td>\n",
       "      <td>0.8492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.725055  0.764273  0.498144        0.8350\n",
       "1  0.489142  0.830491  0.451548        0.8436\n",
       "2  0.444989  0.844291  0.433190        0.8492"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
