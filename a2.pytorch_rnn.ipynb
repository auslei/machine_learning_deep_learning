{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706b086c-1458-46d1-a1ca-55f7bf220ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import portalocker\n",
    "import torchtext.datasets as datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "# Download the IMDB dataset\n",
    "train_iter, test_iter = datasets.IMDB()\n",
    "\n",
    "# Create a tokenizer\n",
    "\n",
    "#tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149155eb-3824-4816-91d4-73cf4d22bbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23e1106-5667-4c63-af1b-69332a0b855c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/auslei/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/share/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/machine_learning_deep_learning/.env/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/machine_learning_deep_learning/.env/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/auslei/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/share/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[1;32m      6\u001b[0m punct \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(string\u001b[39m.\u001b[39mpunctuation)                 \u001b[39m# remove punctuations\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))    \u001b[39m# remove stopwords\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Define a function to preprocess the text data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_data\u001b[39m(data):\n",
      "File \u001b[0;32m~/dev/machine_learning_deep_learning/.env/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/dev/machine_learning_deep_learning/.env/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/dev/machine_learning_deep_learning/.env/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/machine_learning_deep_learning/.env/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/auslei/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/share/nltk_data'\n    - '/home/auslei/dev/machine_learning_deep_learning/.env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "punct = set(string.punctuation)                 # remove punctuations\n",
    "stop_words = set(stopwords.words('english'))    # remove stopwords\n",
    "\n",
    "# Define a function to preprocess the text data\n",
    "def process_data(data):\n",
    "    label = data[0]\n",
    "    text = data[1]\n",
    "    tokens = preprocess(text)\n",
    "    return label, tokens\n",
    "\n",
    "# Define a function to tokenize and remove stop words\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in punct]\n",
    "    return tokens\n",
    "\n",
    "# Create vocabulary from the training data\n",
    "mapped_train_iter = map(process_data, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7c5e1-05fe-4e37-93d0-d27abd3292e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an iterator that yields the tokens, this is then used to build vocab\n",
    "def token_iterator(sentences):\n",
    "    for _, s in sentences:\n",
    "        yield s\n",
    "\n",
    "# tokens to index\n",
    "def tokens_to_idex(vocab, sentences):\n",
    "    token_idx = [torch.LongTensor(vocab.lookup_indices(preprocess(sentence))) for sentence in sentences]\n",
    "    return token_idx\n",
    "        \n",
    "# Define a collate function to pad the sequences to the same length\n",
    "def collate_fn(batch):\n",
    "    labels, texts = zip(*batch)\n",
    "    texts = [torch.LongTensor([vocab[token.lower()] for token in text]) for text in texts] #sentence tokens to index\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64767f31-55bb-428f-ad48-81b6148ca42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(token_iterator(mapped_train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(0)\n",
    "# Print the size of the vocabulary\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade92d2-cfc0-4fb8-9ea3-524d7dc1751b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab.lookup_indices([\"dsdfa\", \"item\"])\n",
    "vocab.lookup_tokens([69940, 6905])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcf1c4-2a55-4224-b085-c16fd6be2515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_iter, batch_size=64, shuffle=True, collate_fn=collate_fn) #create a train loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39704f05-b0d3-4fd9-ad2f-dd20040e0bb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24cf72-97e6-4c3f-9295-49474a913210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define LSTM-based model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_last = hidden[-1, :]\n",
    "        return torch.sigmoid(self.fc(hidden_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50d4a8-c97c-4ccf-8637-3b24a32ab7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# define model hyperparameters\n",
    "INPUT_DIM = len(vocab) # input dimension will be the vocab\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# initialize model\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7ee75-5d65-4254-94a4-f266f22d23c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#train_len = len(sentence_iterator(train_iter))\n",
    "\n",
    "# define function to calculate accuracy\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# train the model\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        predictions = model(texts)\n",
    "        #print(texts.shape, predictions.shape, labels.shape)\n",
    "        #print(texts.dtype, predictions.dtype, labels.dtype)\n",
    "        loss = criterion(predictions, labels.reshape(-1, 1).to(torch.float32))\n",
    "        #acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        #epoch_acc += acc.item()\n",
    "    \n",
    "    #epoch_loss /= train_len\n",
    "    #epoch_acc /= train_len\n",
    "    print(f'Epoch {epoch}: Loss={epoch_loss:.3f}')#, Accuracy={epoch_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f2c09-ad20-44d6-bfc8-13fe92e09bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d81fd0-409a-4db1-94e2-6b635c53ec0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input, target.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d980d09-f1d3-4bef-adc5-57671716db86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 1, requires_grad=True)\n",
    "target = torch.randn(3, 1).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284d1ef-4f66-473c-9516-495ea9bb5238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e5e88-a9c3-4e39-be32-be50082905c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
