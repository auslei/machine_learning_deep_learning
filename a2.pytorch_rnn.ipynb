{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "706b086c-1458-46d1-a1ca-55f7bf220ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import portalocker\n",
    "import torchtext.datasets as datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "# Download the IMDB dataset\n",
    "train_iter, test_iter = datasets.IMDB()\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "149155eb-3824-4816-91d4-73cf4d22bbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f23e1106-5667-4c63-af1b-69332a0b855c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "punct = set(string.punctuation)                 # remove punctuations\n",
    "stop_words = set(stopwords.words('english'))    # remove stopwords\n",
    "\n",
    "# Define a function to preprocess the text data\n",
    "def process_data(data):\n",
    "    label = data[0]\n",
    "    text = data[1]\n",
    "    tokens = preprocess(text)\n",
    "    return label, tokens\n",
    "\n",
    "# Define a function to tokenize and remove stop words\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in punct]\n",
    "    return tokens\n",
    "\n",
    "# Create vocabulary from the training data\n",
    "mapped_train_iter = map(process_data, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ba7c5e1-05fe-4e37-93d0-d27abd3292e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an iterator that yields the tokens, this is then used to build vocab\n",
    "def token_iterator(sentences):\n",
    "    for _, s in sentences:\n",
    "        yield s\n",
    "\n",
    "# tokens to index\n",
    "def tokens_to_idex(vocab, sentences):\n",
    "    token_idx = [torch.LongTensor(vocab.lookup_indices(preprocess(sentence))) for sentence in sentences]\n",
    "    return token_idx\n",
    "        \n",
    "# Define a collate function to pad the sequences to the same length\n",
    "def collate_fn(batch):\n",
    "    labels, texts = zip(*batch)\n",
    "    texts = [torch.LongTensor([vocab[token.lower()] for token in text]) for text in texts] #sentence tokens to index\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64767f31-55bb-428f-ad48-81b6148ca42d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 88455\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(token_iterator(mapped_train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(0)\n",
    "# Print the size of the vocabulary\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ade92d2-cfc0-4fb8-9ea3-524d7dc1751b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ds', 'item']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices([\"dsdfa\", \"item\"])\n",
    "vocab.lookup_tokens([69940, 6905])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62fcf1c4-2a55-4224-b085-c16fd6be2515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_iter, batch_size=64, shuffle=True, collate_fn=collate_fn) #create a train loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39704f05-b0d3-4fd9-ad2f-dd20040e0bb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c24cf72-97e6-4c3f-9295-49474a913210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define LSTM-based model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_last = hidden[-1, :]\n",
    "        return torch.sigmoid(self.fc(hidden_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6d50d4a8-c97c-4ccf-8637-3b24a32ab7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# define model hyperparameters\n",
    "INPUT_DIM = len(vocab) # input dimension will be the vocab\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# initialize model\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7ee75-5d65-4254-94a4-f266f22d23c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#train_len = len(sentence_iterator(train_iter))\n",
    "\n",
    "# define function to calculate accuracy\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# train the model\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for i, (texts, labels) in enumerate(train_loader):\n",
    "        predictions = model(texts)\n",
    "        #print(texts.shape, predictions.shape, labels.shape)\n",
    "        #print(texts.dtype, predictions.dtype, labels.dtype)\n",
    "        loss = criterion(predictions, labels.reshape(-1, 1).to(torch.float32))\n",
    "        #acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        #epoch_acc += acc.item()\n",
    "    \n",
    "    epoch_loss /= train_len\n",
    "    #epoch_acc /= train_len\n",
    "    print(f'Epoch {epoch}: Loss={epoch_loss:.3f}')#, Accuracy={epoch_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f2c09-ad20-44d6-bfc8-13fe92e09bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d81fd0-409a-4db1-94e2-6b635c53ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d980d09-f1d3-4bef-adc5-57671716db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 1, requires_grad=True)\n",
    "target = torch.randn(3, 1).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284d1ef-4f66-473c-9516-495ea9bb5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e5e88-a9c3-4e39-be32-be50082905c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
