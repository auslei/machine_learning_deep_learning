{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients using AutoDiff\n",
    "\n",
    "This section is to understand how to compute gradients automatically using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.000003007075065, 10.000000003174137)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "# of course we can find the derivative at specific points by approximating using a small delta\n",
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps, (f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using tensorflow autodiff\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient is automatically ereased when gradient is called\n",
    "##--> tape.gradient(z, w1)\n",
    "\n",
    "# another way is to set persistent=True so the gradient is over ereased, but \n",
    "# need to remember to delete the tape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "g1 = tape.gradient(z, [w1])\n",
    "g2 = tape.gradient(z, [w2])\n",
    "\n",
    "g1, g2\n",
    "\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normally only tracks variables. but can also be set to track constant.\n",
    "## this can be useful for some case. for example, implement a regularisation loss that \n",
    "## panelises activations that very a lot when inputs vary little\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# test_size is default to 0.25\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_valid = StandardScaler().fit_transform(X_valid)\n",
    "X_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 161\n",
      "Trainable params: 161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "363/363 [==============================] - 0s 802us/step - loss: 2.3630 - mse: 2.3630 - val_loss: 0.8399 - val_mse: 0.8399\n",
      "Epoch 2/40\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.7489 - mse: 0.7489 - val_loss: 0.5844 - val_mse: 0.5844\n",
      "Epoch 3/40\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5571 - mse: 0.5571 - val_loss: 0.5690 - val_mse: 0.5690\n",
      "Epoch 4/40\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.4744 - mse: 0.4744 - val_loss: 0.5726 - val_mse: 0.5726\n",
      "Epoch 5/40\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4419 - mse: 0.4419 - val_loss: 0.5572 - val_mse: 0.5572\n",
      "Epoch 6/40\n",
      "363/363 [==============================] - 0s 620us/step - loss: 0.4246 - mse: 0.4246 - val_loss: 0.5781 - val_mse: 0.5781\n",
      "Epoch 7/40\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.4112 - mse: 0.4112 - val_loss: 0.5455 - val_mse: 0.5455\n",
      "Epoch 8/40\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.4051 - mse: 0.4051 - val_loss: 0.5627 - val_mse: 0.5627\n",
      "Epoch 9/40\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.3998 - mse: 0.3998 - val_loss: 0.5494 - val_mse: 0.5494\n",
      "Epoch 10/40\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3927 - mse: 0.3927 - val_loss: 0.5956 - val_mse: 0.5956\n",
      "Epoch 11/40\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3877 - mse: 0.3877 - val_loss: 0.5522 - val_mse: 0.5522\n",
      "Epoch 12/40\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3857 - mse: 0.3857 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 13/40\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.3800 - mse: 0.3800 - val_loss: 0.5770 - val_mse: 0.5770\n",
      "Epoch 14/40\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3769 - mse: 0.3769 - val_loss: 0.5770 - val_mse: 0.5770\n",
      "Epoch 15/40\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3733 - mse: 0.3733 - val_loss: 0.5949 - val_mse: 0.5949\n",
      "Epoch 16/40\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.3697 - mse: 0.3697 - val_loss: 0.6018 - val_mse: 0.6018\n",
      "Epoch 17/40\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.3695 - mse: 0.3695 - val_loss: 0.5850 - val_mse: 0.5850\n",
      "Epoch 18/40\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.3684 - mse: 0.3684 - val_loss: 0.5944 - val_mse: 0.5944\n",
      "Epoch 19/40\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3691 - mse: 0.3691 - val_loss: 0.6058 - val_mse: 0.6058\n",
      "Epoch 20/40\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3614 - mse: 0.3614 - val_loss: 0.5774 - val_mse: 0.5774\n",
      "Epoch 21/40\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.3642 - mse: 0.3642 - val_loss: 0.5936 - val_mse: 0.5936\n",
      "Epoch 22/40\n",
      "363/363 [==============================] - 0s 615us/step - loss: 0.3624 - mse: 0.3624 - val_loss: 0.5777 - val_mse: 0.5777\n",
      "Epoch 23/40\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.3589 - mse: 0.3589 - val_loss: 0.6103 - val_mse: 0.6103\n",
      "Epoch 24/40\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3551 - mse: 0.3551 - val_loss: 0.5965 - val_mse: 0.5965\n",
      "Epoch 25/40\n",
      "363/363 [==============================] - 0s 537us/step - loss: 0.3573 - mse: 0.3573 - val_loss: 0.6218 - val_mse: 0.6218\n",
      "Epoch 26/40\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3534 - mse: 0.3534 - val_loss: 0.6245 - val_mse: 0.6245\n",
      "Epoch 27/40\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.3485 - mse: 0.3485 - val_loss: 0.6638 - val_mse: 0.6638\n",
      "Epoch 28/40\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3481 - mse: 0.3481 - val_loss: 0.6345 - val_mse: 0.6345\n",
      "Epoch 29/40\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3470 - mse: 0.3470 - val_loss: 0.6549 - val_mse: 0.6549\n",
      "Epoch 30/40\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.3455 - mse: 0.3455 - val_loss: 0.6722 - val_mse: 0.6722\n",
      "Epoch 31/40\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3469 - mse: 0.3469 - val_loss: 0.6563 - val_mse: 0.6563\n",
      "Epoch 32/40\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3459 - mse: 0.3459 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 33/40\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.3502 - mse: 0.3502 - val_loss: 0.6538 - val_mse: 0.6538\n",
      "Epoch 34/40\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3445 - mse: 0.3445 - val_loss: 0.7140 - val_mse: 0.7140\n",
      "Epoch 35/40\n",
      "363/363 [==============================] - 0s 615us/step - loss: 0.3410 - mse: 0.3410 - val_loss: 0.6793 - val_mse: 0.6793\n",
      "Epoch 36/40\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.3403 - mse: 0.3403 - val_loss: 0.7360 - val_mse: 0.7360\n",
      "Epoch 37/40\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.3394 - mse: 0.3394 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 38/40\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3367 - mse: 0.3367 - val_loss: 0.7083 - val_mse: 0.7083\n",
      "Epoch 39/40\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.3356 - mse: 0.3356 - val_loss: 0.6437 - val_mse: 0.6437\n",
      "Epoch 40/40\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3383 - mse: 0.3383 - val_loss: 0.7112 - val_mse: 0.7112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b61cfae340>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "l2_reg = keras.regularizers.l2(1.0)\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    Dense(16, input_shape = X_train.shape[1:] , activation = 'relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "# how we would compile and run the model\n",
    "model.compile(optimizer=\"adam\", loss = 'mse', metrics=['mse'])\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_batch(X, y, batch_size = 30):\n",
    "    idx = np.random.randint(len(X), size = batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics = \"-\".join([f\"{m.name}: {m.result()}\"\n",
    "                        for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end = end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Adam(lr = 0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "11610/11610 - mean: 1.3216079473495483-mean_absolute_error: 0.8897098302841187\n",
      "Epoch: 2/5\n",
      "11610/11610 - mean: 1.3708149194717407-mean_absolute_error: 0.910310685634613\n",
      "Epoch: 3/5\n",
      "11610/11610 - mean: 1.310929298400879-mean_absolute_error: 0.8860278129577637\n",
      "Epoch: 4/5\n",
      "11610/11610 - mean: 1.3164016008377075-mean_absolute_error: 0.8873282670974731\n",
      "Epoch: 5/5\n",
      "11610/11610 - mean: 1.328629970550537-mean_absolute_error: 0.8908034563064575\n"
     ]
    }
   ],
   "source": [
    "# 2 loops, top is for epochs, and second is for steps (see above)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch: {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)               # grab a random batch\n",
    "        with tf.GradientTape() as tape:                                 # gradient tape created for loss\n",
    "            y_pred = model(X_batch, training = True)                    # calculate prediction\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))        # calculate loss for the batch\n",
    "            loss = tf.add_n([main_loss] + model.losses)                 # add other losses (for regularisation for example)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)      # get gradients of all trainable variables\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # apply gradients, smart, as this can be optimiser specific such as learning rate, momentum, etc\n",
    "        mean_loss(loss)             # calculate mean loss and meatrics (over the current epoch)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in tensorflow/9.1_auto_diff.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 37e6f26] initial commit\n",
      " 1 file changed, 56 insertions(+), 5 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/auslei/python.git\n",
      "   3390cbd..37e6f26  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add 9.1_auto_diff.ipynb\n",
    "!git commit -m \"initial commit\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(1., dtype = \"float32\")\n",
    "b = tf.constant(2., dtype = \"float32\")\n",
    "\n",
    "[a] + [b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow fuction and Graph\n",
    "\n",
    "In general TF function is much faster than vanila function as it is graph optimised.\n",
    "\n",
    "**note**\n",
    "if calling tf function with numeric value, a graph will be generated for each value, it can be extremely expensive. when passing in only tensors, the tensor will be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x**3\n",
    "\n",
    "tf_cube = tf.function(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 ns ± 0.601 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.7 µs ± 492 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def tf__cube(x):\\n    with ag__.FunctionScope('cube', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\\n        do_return = False\\n        retval_ = ag__.UndefinedReturnValue()\\n        try:\\n            do_return = True\\n            retval_ = (ag__.ld(x) ** 3)\\n        except:\\n            do_return = False\\n            raise\\n        return fscope.ret(retval_, do_return)\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.autograph.to_code(tf_cube.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_with_tf",
   "language": "python",
   "name": "ml_with_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
